{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "051f17a4-8f72-439f-b470-a2738c907a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90188a6-ebd1-48d5-9ce0-454abbeb6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa4cf84d-23f2-41d7-81b4-09c8006aa8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92baf6a4-7a54-4b39-a957-c21c1b8c01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(filename, page_numbers=None, min_line_length=10):\n",
    "    '''从 PDF 文件中（按指定页码）提取文字'''\n",
    "    paragraphs = []\n",
    "    buffer = ''\n",
    "    full_text = ''\n",
    "    # 提取全部文本\n",
    "    for i, page_layout in enumerate(extract_pages(filename)):\n",
    "        # 如果指定了页码范围，跳过范围外的页\n",
    "        if page_numbers is not None and i not in page_numbers:\n",
    "            continue\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                full_text += element.get_text() + '\\n'\n",
    "    # 按空行分隔，将文本重新组织成段落\n",
    "    # 但上面将所有的文本框中的文本用换行符切割，并不是pdf原生的段落\n",
    "    lines = full_text.split('\\n')\n",
    "    for text in lines:\n",
    "        # 当遇到一个比较小的字符长度文本（通常是标题），那么就把之前积累的文本内容作为一个段落 \n",
    "        if len(text) >= min_line_length:\n",
    "            # 如果text不以'-'结尾，则在text前面添加一个空格，即单词和单词之间的空格，否则将'-'去除\n",
    "            buffer += (' '+text) if not text.endswith('-') else text.strip('-')\n",
    "        elif buffer:\n",
    "            paragraphs.append(buffer)\n",
    "            buffer = ''\n",
    "    if buffer:\n",
    "        paragraphs.append(buffer)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd2a127a-e5f0-45d4-a29c-71ad2b8d83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = extract_text_from_pdf(\"llama2.pdf\", min_line_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee61fa4e-b848-4f6d-9104-b1f43436306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "\n",
      " Hugo Touvron∗ Louis Martin† Kevin Stone† Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic Sergey Edunov Thomas Scialom∗\n",
      "\n",
      " In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our ﬁne-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based onour human evaluations for helpfulness and safety, may be a suitable substitute for closed source models. We provide a detailed description of our approach to ﬁne-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.\n",
      "\n",
      " ∗Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
      "\n",
      " Contributions for all the authors can be found in Section A.1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in paragraphs[:5]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15ac6adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a003ad0-dae9-4cc6-9b5e-7a1edd058cea",
   "metadata": {},
   "source": [
    "## LLM封装接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52e07bad-50d6-43e5-96c1-8484c5bb3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f36b1e7-0e5b-4044-91ef-71540296058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08bebf42-d7fa-4786-94d0-84b2e3cf860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt_template, **kwargs):\n",
    "    # **kwargs 是一个包含所有参数的 Python 字典。\n",
    "    inputs = {}\n",
    "    for k, v in kwargs.items():\n",
    "        if isinstance(v, list) and all(isinstance(elem, str) for elem in v):\n",
    "            # 这个条件判断用于检查当前参数值 v 是否是一个字符串列表。如果 v 是一个列表，并且该列表中的所有元素都是字符串，则返回 True。\n",
    "            val = '\\n\\n'.join(v)\n",
    "            # 如果上面的条件为真，那么将列表中的所有字符串用两个换行符 \\n\\n 连接起来，得到一个新的字符串 val。\n",
    "        else:\n",
    "            val = v\n",
    "        inputs[k] = val\n",
    "    return prompt_template.format(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec1538a6-fbf2-4d5d-8578-d743b0436425",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "\n",
    "已知信息:\n",
    "{context}\n",
    "\n",
    "用户问：\n",
    "{query}\n",
    "\n",
    "如果已知信息不包含用户问题的答案，或者已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "请不要输出已知信息中不包含的信息或答案。\n",
    "请用中文回答用户问题。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9954f3f7-ea09-4dbe-ac81-f28ead871769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Prompt===\n",
      "\n",
      "你是一个问答机器人。\n",
      "你的任务是根据下述给定的已知信息回答用户问题。\n",
      "\n",
      "已知信息:\n",
      "Llama 2 is a large language model developed by Meta (formerly known as Facebook). It is part of the Llama (Large Language Model Meta AI) family and is designed to generate human-like text based on the input it receives.\n",
      "\n",
      "用户问：\n",
      "Do you know llama 2?\n",
      "\n",
      "如果已知信息不包含用户问题的答案，或者已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
      "请不要输出已知信息中不包含的信息或答案。\n",
      "请用中文回答用户问题。\n",
      "\n",
      "===回复===\n",
      "是的，我知道Llama 2是由Meta（原名Facebook）开发的一种大型语言模型，它是Llama（Large Language Model Meta AI）家族的一部分，旨在根据接收到的输入生成类似人类的文本。\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Do you know llama 2?\"\n",
    "# user_query = \"HEllo\"\n",
    "\n",
    "# 1. 检索\n",
    "search_results = \"Llama 2 is a large language model developed by Meta (formerly known as Facebook). It is part of the Llama (Large Language Model Meta AI) family and is designed to generate human-like text based on the input it receives.\"\n",
    "\n",
    "# 2. 构建 Prompt\n",
    "# build_prompt中的后两个参数对应prompt_template中的{context}和{query}\n",
    "prompt = build_prompt(prompt_template, context = search_results, query=user_query)\n",
    "print(\"===Prompt===\")\n",
    "print(prompt)\n",
    "\n",
    "# 3. 调用 LLM\n",
    "response = get_completion(prompt)\n",
    "\n",
    "print(\"===回复===\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad12a168-8b0f-47f3-a0ea-2be48831b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为了演示方便，只取两页（第一章）\n",
    "paragraphs = extract_text_from_pdf(\n",
    "    \"llama2.pdf\",\n",
    "    page_numbers=[2, 3],\n",
    "    min_line_length=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d3e05657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(len(paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c02796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面的对于文档的分割粒度太大，有可能导致检索不精确，粒度太小又可能信息不全\n",
    "# 此外，问题的答案也有可能跨越两个句子\n",
    "# 因此，采用部分重叠式的切割文本，使上下文更完整\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import json\n",
    "\n",
    "# sent_tokenize()方法是针对英文的实现，并非中文\n",
    "def split_text(paragraphs, chunk_size=300, overlap_size=100):\n",
    "    '''按指定 chunk_size 和 overlap_size 交叠割文本'''\n",
    "    # sent_tokenize(p)把每一个段落分割成句子\n",
    "    # 每个句子去除前后空白字符并将所有句子合并到sentences列表中\n",
    "    sentences = [s.strip() for p in paragraphs for s in sent_tokenize(p)]\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        chunk = sentences[i]\n",
    "        overlap = ''\n",
    "        prev_len = 0\n",
    "        prev = i - 1\n",
    "        # 向前计算重叠部分\n",
    "        # 如果前一个句子height + 当前overlap的句子仍然小于overlap_size，则将前一个句子加到overlap中\n",
    "        while prev >= 0 and len(sentences[prev])+len(overlap) <= overlap_size:\n",
    "            overlap = sentences[prev] + ' ' + overlap\n",
    "            prev -= 1\n",
    "        chunk = overlap+chunk\n",
    "        next = i + 1\n",
    "        # 向后计算当前chunk\n",
    "        # 向后不断添加句子，直至满足设定的chunk_size\n",
    "        while next < len(sentences) and len(sentences[next])+len(chunk) <= chunk_size:\n",
    "            chunk = chunk + ' ' + sentences[next]\n",
    "            next += 1\n",
    "        chunks.append(chunk)\n",
    "        i = next\n",
    "        # 最终返回的chunks中的每一个chunk都包含多个句子，这里就不再适用段落分割了\n",
    "    return chunks\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0a91b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果报Lookup Error，需要下载punkt\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7da26d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把之前分割好的段落再分成部分重叠的chunks\n",
    "chunks = split_text(paragraphs, 300, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84c365ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1: Helpfulness human evaluation results for Llama 2-Chat compared to other open-source and closed-source models. Human raters compared model generations on ~4k prompts consisting of both single and multi-turn prompts. The 95% conﬁdence intervals for this evaluation are between 1% and 2%.\n",
      "\n",
      "The 95% conﬁdence intervals for this evaluation are between 1% and 2%. More details in Section 3.4.2.\n",
      "\n",
      "More details in Section 3.4.2. While reviewing these results, it is important to note that human evaluations can be noisy due to limitations of the prompt set, subjectivity of the review guidelines, subjectivity of individual raters, and the inherent diﬃculty of comparing generations.\n",
      "\n",
      "Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed baselines and Llama 2-Chat, according to GPT 4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4.\n",
      "\n",
      "Green area indicates our model is better according to GPT-4. To remove ties, we used win/(win + loss). The orders in which the model responses are presented to GPT-4 are randomly swapped to alleviate bias. 1 Introduction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 可以看到不同的trunk之间确实有了重叠\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9879e83e-743e-4399-bbb9-21a76597fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts, model=\"text-embedding-ada-002\", dimensions=None):\n",
    "    '''封装 OpenAI 的 Embedding 模型接口'''\n",
    "    if model == \"text-embedding-ada-002\":\n",
    "        dimensions = None\n",
    "    if dimensions:\n",
    "        data = client.embeddings.create(\n",
    "            input=texts, model=model, dimensions=dimensions).data\n",
    "    else:\n",
    "        data = client.embeddings.create(input=texts, model=model).data\n",
    "    return [x.embedding for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ac7d94e-cc9d-4c1e-8a8a-bfb85846825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "class MyVectorDBConnector:\n",
    "    def __init__(self, collection_name, embedding_fn):\n",
    "        chroma_client = chromadb.Client(Settings(allow_reset=True))\n",
    "\n",
    "        # 为了演示，实际不需要每次 reset()\n",
    "        chroma_client.reset()\n",
    "\n",
    "        # 创建一个 collection\n",
    "        self.collection = chroma_client.get_or_create_collection(\n",
    "            name=collection_name)\n",
    "        self.embedding_fn = embedding_fn\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''向 collection 中添加文档与向量'''\n",
    "        self.collection.add(\n",
    "            embeddings=self.embedding_fn(documents),  # 每个文档的向量\n",
    "            documents=documents,  # 文档的原文\n",
    "            ids=[f\"id{i}\" for i in range(len(documents))]  # 每个文档的 id\n",
    "        )\n",
    "\n",
    "    def search(self, query, top_n):\n",
    "        '''检索向量数据库'''\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=self.embedding_fn([query]),\n",
    "            n_results=top_n\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec37708f-ac97-4f2c-af67-3e93b546c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个向量数据库对象\n",
    "vector_db = MyVectorDBConnector(\"demo\", get_embeddings)\n",
    "# 向向量数据库中添加文档，参数可选择paragraphs或chunks\n",
    "# vector_db.add_documents(paragraphs)\n",
    "vector_db.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60b4b225-fbe2-45fb-8e13-9ba2fec8ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query = \"Llama 2有多少参数\"\n",
    "user_query = \"Llama 2有商业许可协议吗？\"\n",
    "results = vector_db.search(user_query, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7c9e267-dd71-4a2d-a821-262449903704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Llama 2-Chat, a ﬁne-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net beneﬁt to society.\n",
      "\n",
      "We are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.\n",
      "\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "We believe that the open release of LLMs, when done safely, will be a net beneﬁt to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).\n",
      "\n",
      "Figure 2: Win-rate % for helpfulness andsafety between commercial-licensed baselines and Llama 2-Chat, according to GPT 4. To complement the human evaluation, we used a more capable model, not subject to our own guidance. Green area indicates our model is better according to GPT-4.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for para in results['documents'][0]:\n",
    "    print(para+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a1b83-fbe9-4dd4-8631-7fdc8296fdee",
   "metadata": {},
   "source": [
    "上面只是构建了向量数据库，然后根据user_query从数据库中检索出前几个段。\n",
    "接下来不仅要检索，还要把检索的结果放到prompt，最后再喂给LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f44b4eb8-385d-43fa-a0ef-2c97b9729d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_Bot:\n",
    "    def __init__(self, vector_db, llm_api, n_results=2):\n",
    "        self.vector_db = vector_db\n",
    "        self.llm_api = llm_api\n",
    "        self.n_results = n_results\n",
    "\n",
    "    def chat(self, user_query):\n",
    "        # 1. 检索\n",
    "        search_results = self.vector_db.search(user_query, self.n_results)\n",
    "\n",
    "        # 2. 构建 Prompt\n",
    "        prompt = build_prompt(\n",
    "            prompt_template, context=search_results['documents'][0], query=user_query)\n",
    "\n",
    "        # 3. 调用 LLM\n",
    "        response = self.llm_api(prompt)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f0bd295-b6fa-4be8-8aca-d056bea6660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====搜索结果====\n",
      "2. Llama 2-Chat, a ﬁne-tuned version of Llama 2 that is optimized for dialogue use cases. We release variants of this model with 7B, 13B, and 70B parameters as well. We believe that the open release of LLMs, when done safely, will be a net beneﬁt to society.\n",
      "\n",
      "We are releasing the following models to the general public for research and commercial use‡: 1. Llama 2, an updated version of Llama 1, trained on a new mix of publicly available data.\n",
      "\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "We also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.\n",
      "\n",
      "We believe that the open release of LLMs, when done safely, will be a net beneﬁt to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).\n",
      "\n",
      "====回复====\n",
      "llama 2是开源的。\n"
     ]
    }
   ],
   "source": [
    "# 创建一个RAG机器人\n",
    "bot = RAG_Bot(\n",
    "    vector_db,\n",
    "    llm_api=get_completion\n",
    ")\n",
    "\n",
    "user_query = \"llama 2是什么？\"\n",
    "user_query = \"llama 2有商用许可协议吗\"\n",
    "user_query = \"llama 2是开源的吗？\"\n",
    "\n",
    "print(\"====搜索结果====\")\n",
    "search_results = bot.vector_db.search(user_query, 5)\n",
    "for doc in search_results['documents'][0]:\n",
    "    print(doc + \"\\n\")\n",
    "\n",
    "print(\"====回复====\")\n",
    "response = bot.chat(user_query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbe4ee",
   "metadata": {},
   "source": [
    "# RAG的改进方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fc646",
   "metadata": {},
   "source": [
    "## 改进一：检索后排序(Rerank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccb4f67",
   "metadata": {},
   "source": [
    "**问题**： 最佳答案并不一定在排在检索的前面\n",
    "\n",
    "**解决方案**：把top k调大一些\n",
    "\n",
    "但该方法又会有新的问题，检索的内容变多了，prompt会变大，耗费的tokens更多。这时可以采取检索后再进行一次排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "566a7141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed0e2e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kory/miniconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', max_length=512) # 英文，模型较小\n",
    "# model = CrossEncoder('BAAI/bge-reranker-large', max_length=512) # 多语言，国产，模型较大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "426d160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.613734245300293\tWe believe that the open release of LLMs, when done safely, will be a net beneﬁt to society. Like all LLMs, Llama 2 is a new technology that carries potential risks with use (Bender et al., 2021b; Weidinger et al., 2021; Solaiman et al., 2023).\n",
      "\n",
      "5.310719013214111\tIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, Llama 2 and Llama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested, Llama 2-Chat models generally perform better than existing open-source models.\n",
      "\n",
      "4.709953308105469\tWe provide a responsible use guide¶ and code examples‖ to facilitate the safe deployment of Llama 2 and Llama 2-Chat. More details of our responsible release strategy can be found in Section 5.3.\n",
      "\n",
      "4.543964385986328\tWe also share novel observations we made during the development of Llama 2 and Llama 2-Chat, such as the emergence of tool usage and temporal organization of knowledge. Figure 3: Safety human evaluation results for Llama 2-Chat compared to other open-source and closed source models.\n",
      "\n",
      "4.0338897705078125\tAdditionally, these safety evaluations are performed using content standards that are likely to be biased towards the Llama 2-Chat models. We are releasing the following models to the general public for research and commercial use‡: 1.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"how safe is llama 2\"\n",
    "search_results = bot.vector_db.search(user_query, 5)\n",
    "\n",
    "# 预测模型的输入和检索到的文档的相关性得分\n",
    "scores = model.predict([(user_query, doc)\n",
    "                       for doc in search_results['documents'][0]])\n",
    "# 按得分排序\n",
    "sorted_list = sorted(\n",
    "    zip(scores, search_results['documents'][0]), key=lambda x: x[0], reverse=True)\n",
    "for score, doc in sorted_list:\n",
    "    print(f\"{score}\\t{doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec0964a",
   "metadata": {},
   "source": [
    "## 改进二：混合检索（Hybrid Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b747e2",
   "metadata": {},
   "source": [
    "**问题**：在实际生产中，传统的关键字检索（稀疏表示）与向量表示（稠密表示）各有优劣。\n",
    "\n",
    "文档中包含很长的专有名词，关键字检索往往更精准而向量检索容易引入概念混淆。\n",
    "\n",
    "例如：“小细胞肺癌”和“非小细胞肺癌”。 只有一字之差，二者向量相似度较高，但二者却是不同的癌症\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186cad45",
   "metadata": {},
   "source": [
    "所以，有时候我们需要结合不同的检索算法，来达到比单一检索算法更优的效果。这就是**混合检索**。\n",
    "\n",
    "混合检索的核心是，综合文档 $d$ 在不同检索算法下的排序名次（rank），为其生成最终排序。\n",
    "\n",
    "一个最常用的算法叫 **Reciprocal Rank Fusion（RRF）**\n",
    "\n",
    "$rrf(d)=\\sum_{a\\in A}\\frac{1}{k+rank_a(d)}$\n",
    "\n",
    "其中 $A$ 表示所有使用的检索算法的集合，$rank_a(d)$ 表示使用算法 $a$ 检索时，文档 $d$ 的排序，$k$ 是个常数。\n",
    "\n",
    "很多向量数据库都支持混合检索，比如 [Weaviate](https://weaviate.io/blog/hybrid-search-explained)、[Pinecone](https://www.pinecone.io/learn/hybrid-search-intro/) 等。也可以根据上述原理自己实现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b0405",
   "metadata": {},
   "source": [
    "1. 基于关键字检索的排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa796479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch7 import Elasticsearch, helpers\n",
    "from chinese_utils import to_keywords  # 使用中文的关键字提取函数\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "class MyEsConnector:\n",
    "    def __init__(self, es_client, index_name, keyword_fn):\n",
    "        self.es_client = es_client\n",
    "        self.index_name = index_name\n",
    "        self.keyword_fn = keyword_fn\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''文档灌库'''\n",
    "        if self.es_client.indices.exists(index=self.index_name):\n",
    "            self.es_client.indices.delete(index=self.index_name)\n",
    "        self.es_client.indices.create(index=self.index_name)\n",
    "        actions = [\n",
    "            {\n",
    "                \"_index\": self.index_name,\n",
    "                \"_source\": {\n",
    "                    \"keywords\": self.keyword_fn(doc),\n",
    "                    \"text\": doc,\n",
    "                    \"id\": f\"doc_{i}\"\n",
    "                }\n",
    "            }\n",
    "            for i, doc in enumerate(documents)\n",
    "        ]\n",
    "        helpers.bulk(self.es_client, actions)\n",
    "        time.sleep(1)\n",
    "\n",
    "    def search(self, query_string, top_n=3):\n",
    "        '''检索'''\n",
    "        search_query = {\n",
    "            \"match\": {\n",
    "                \"keywords\": self.keyword_fn(query_string)\n",
    "            }\n",
    "        }\n",
    "        res = self.es_client.search(\n",
    "            index=self.index_name, query=search_query, size=top_n)\n",
    "        return {\n",
    "            hit[\"_source\"][\"id\"]: {\n",
    "                \"text\": hit[\"_source\"][\"text\"],\n",
    "                \"rank\": i,\n",
    "            }\n",
    "            for i, hit in enumerate(res[\"hits\"][\"hits\"])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79883f8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "ConnectionError(<urllib3.connection.HTTPConnection object at 0x2ecd4a840>: Failed to establish a new connection: [Errno 61] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x2ecd4a840>: Failed to establish a new connection: [Errno 61] Connection refused)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    175\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/elasticsearch7/connection/http_urllib3.py:255\u001b[0m, in \u001b[0;36mUrllib3HttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    253\u001b[0m     request_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent-encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 255\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[1;32m    256\u001b[0m     method, url, body, retries\u001b[38;5;241m=\u001b[39mRetry(\u001b[38;5;28;01mFalse\u001b[39;00m), headers\u001b[38;5;241m=\u001b[39mrequest_headers, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    258\u001b[0m response_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    259\u001b[0m     header\u001b[38;5;241m.\u001b[39mlower(): value \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    260\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:801\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    799\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 801\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[1;32m    802\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39me, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    804\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/util/retry.py:527\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m error:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m# Disabled, indicate to re-raise the error.\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m six\u001b[38;5;241m.\u001b[39mreraise(\u001b[38;5;28mtype\u001b[39m(error), error, _stacktrace)\n\u001b[1;32m    529\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/packages/six.py:770\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m--> 770\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:715\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 715\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    716\u001b[0m     conn,\n\u001b[1;32m    717\u001b[0m     method,\n\u001b[1;32m    718\u001b[0m     url,\n\u001b[1;32m    719\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[1;32m    720\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    721\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m    722\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[1;32m    723\u001b[0m )\n\u001b[1;32m    725\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connectionpool.py:416\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m         conn\u001b[38;5;241m.\u001b[39mrequest(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhttplib_request_kw)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:244\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers)\u001b[0m\n\u001b[1;32m    243\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _get_default_user_agent()\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28msuper\u001b[39m(HTTPConnection, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(method, url, body, headers, encode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1376\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders(body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1326\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1085\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1088\u001b[0m \n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/http/client.py:1029\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1029\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:205\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 205\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_conn(conn)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x2ecd4a840>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# 文档灌库\u001b[39;00m\n\u001b[1;32m     19\u001b[0m chunks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGLM 2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m es_connector\u001b[38;5;241m.\u001b[39madd_documents(chunks)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 关键字检索\u001b[39;00m\n\u001b[1;32m     23\u001b[0m keyword_search_results \u001b[38;5;241m=\u001b[39m es_connector\u001b[38;5;241m.\u001b[39msearch(query, \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m, in \u001b[0;36mMyEsConnector.add_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''文档灌库'''\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mes_client\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mexists(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_name):\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mes_client\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdelete(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_name)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mes_client\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mcreate(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_name)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/elasticsearch7/client/utils.py:347\u001b[0m, in \u001b[0;36mquery_params.<locals>._wrapper.<locals>._wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    346\u001b[0m         params[p] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(p)\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/elasticsearch7/client/indices.py:371\u001b[0m, in \u001b[0;36mIndicesClient.exists\u001b[0;34m(self, index, params, headers)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m SKIP_IN_PATH:\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty value passed for a required argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransport\u001b[38;5;241m.\u001b[39mperform_request(\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, _make_path(index), params\u001b[38;5;241m=\u001b[39mparams, headers\u001b[38;5;241m=\u001b[39mheaders\n\u001b[1;32m    373\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/elasticsearch7/transport.py:417\u001b[0m, in \u001b[0;36mTransport.perform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;66;03m# Before we make the actual API call we verify the Elasticsearch instance.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_verify_elasticsearch(headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;66;03m# If '_verified_elasticsearch' isn't 'True' then we raise an error.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/elasticsearch7/transport.py:606\u001b[0m, in \u001b[0;36mTransport._do_verify_elasticsearch\u001b[0;34m(self, headers, timeout)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# If we received a connection error and weren't successful\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# anywhere then we re-raise the more appropriate error.\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m info_response:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# Check the information we got back from the index request.\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verified_elasticsearch \u001b[38;5;241m=\u001b[39m _ProductChecker\u001b[38;5;241m.\u001b[39mcheck_product(\n\u001b[1;32m    610\u001b[0m     info_headers, info_response\n\u001b[1;32m    611\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/elasticsearch7/transport.py:569\u001b[0m, in \u001b[0;36mTransport._do_verify_elasticsearch\u001b[0;34m(self, headers, timeout)\u001b[0m\n\u001b[1;32m    566\u001b[0m attempted_conns\u001b[38;5;241m.\u001b[39mappend(conn)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m     _, info_headers, info_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mperform_request(\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGET\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    571\u001b[0m     )\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# Lowercase all the header names for consistency in accessing them.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     info_headers \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    575\u001b[0m         header\u001b[38;5;241m.\u001b[39mlower(): value \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m info_headers\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    576\u001b[0m     }\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/elasticsearch7/connection/http_urllib3.py:280\u001b[0m, in \u001b[0;36mUrllib3HttpConnection.perform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ReadTimeoutError):\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIMEOUT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e), e)\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e), e)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# raise warnings if any from the 'Warnings' header.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m warning_headers \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, ())\n",
      "\u001b[0;31mConnectionError\u001b[0m: ConnectionError(<urllib3.connection.HTTPConnection object at 0x2ecd4a840>: Failed to establish a new connection: [Errno 61] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x2ecd4a840>: Failed to establish a new connection: [Errno 61] Connection refused)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 引入配置文件\n",
    "# ELASTICSEARCH_BASE_URL = os.getenv('ELASTICSEARCH_BASE_URL')\n",
    "# ELASTICSEARCH_PASSWORD = os.getenv('ELASTICSEARCH_PASSWORD')\n",
    "# ELASTICSEARCH_NAME= os.getenv('ELASTICSEARCH_NAME')\n",
    "\n",
    "\n",
    "# es = Elasticsearch(\"http://localhost:9200\")\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "# es = Elasticsearch(\n",
    "#     hosts=[ELASTICSEARCH_BASE_URL],  # 服务地址与端口\n",
    "#     http_auth=(ELASTICSEARCH_NAME, ELASTICSEARCH_PASSWORD),  # 用户名，密码\n",
    "# )\n",
    "\n",
    "\n",
    "# 创建 ES 连接器\n",
    "es_connector = MyEsConnector(es, \"demo_es_rrf\", to_keywords)\n",
    "\n",
    "# 文档灌库\n",
    "chunks = [\"llama 2\", \"ChatGLM 2\"]\n",
    "es_connector.add_documents(chunks)\n",
    "\n",
    "# 关键字检索\n",
    "keyword_search_results = es_connector.search(query, 3)\n",
    "\n",
    "print(json.dumps(keyword_search_results, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b430986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
